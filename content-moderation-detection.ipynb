{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-09-18T11:16:09.536992Z","iopub.execute_input":"2023-09-18T11:16:09.537973Z","iopub.status.idle":"2023-09-18T11:16:09.584113Z","shell.execute_reply.started":"2023-09-18T11:16:09.537919Z","shell.execute_reply":"2023-09-18T11:16:09.582744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nimport pandas as pd\nimport numpy as np\nimport re","metadata":{"execution":{"iopub.status.busy":"2023-09-18T11:16:12.840819Z","iopub.execute_input":"2023-09-18T11:16:12.841310Z","iopub.status.idle":"2023-09-18T11:16:13.675590Z","shell.execute_reply.started":"2023-09-18T11:16:12.841265Z","shell.execute_reply":"2023-09-18T11:16:13.674457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Read the CSV file\ndata = pd.read_csv('/kaggle/input/content-moderation-dataset/testing.csv')\ndata['labels']=data['class'].map({0:\"Adult content\",1:\"Offensive Language\",2:\"Normal Language\"})","metadata":{"execution":{"iopub.status.busy":"2023-09-18T11:16:15.675729Z","iopub.execute_input":"2023-09-18T11:16:15.676467Z","iopub.status.idle":"2023-09-18T11:16:15.830903Z","shell.execute_reply.started":"2023-09-18T11:16:15.676425Z","shell.execute_reply":"2023-09-18T11:16:15.829830Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Clean the text in the second column\ndata['tweet'] = data['tweet'].apply(lambda x: x.lower())  # Convert to lowercase\ndata['tweet'] = data['tweet'].apply(lambda x: re.sub(r'http\\S+|www\\S+|@\\S+|&\\S+|#\\S+|[^A-Za-z0-9]+', ' ', x))  # Remove URLs, mentions, hashtags, and special characters\ndata['tweet'] = data['tweet'].apply(lambda x: re.sub(r'\\s+', ' ', x).strip())  # Remove extra whitespace","metadata":{"execution":{"iopub.status.busy":"2023-09-18T11:16:17.675748Z","iopub.execute_input":"2023-09-18T11:16:17.676816Z","iopub.status.idle":"2023-09-18T11:16:18.215358Z","shell.execute_reply.started":"2023-09-18T11:16:17.676765Z","shell.execute_reply":"2023-09-18T11:16:18.214272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndata[['tweet','labels']]","metadata":{"execution":{"iopub.status.busy":"2023-09-18T11:16:20.839458Z","iopub.execute_input":"2023-09-18T11:16:20.840339Z","iopub.status.idle":"2023-09-18T11:16:20.863743Z","shell.execute_reply.started":"2023-09-18T11:16:20.840291Z","shell.execute_reply":"2023-09-18T11:16:20.862727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Apply Decesion Tree Model","metadata":{"execution":{"iopub.status.busy":"2023-04-11T02:31:29.844333Z","iopub.execute_input":"2023-04-11T02:31:29.844875Z","iopub.status.idle":"2023-04-11T02:31:29.851441Z","shell.execute_reply.started":"2023-04-11T02:31:29.844836Z","shell.execute_reply":"2023-04-11T02:31:29.849530Z"}}},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2023-09-18T11:16:23.683858Z","iopub.execute_input":"2023-09-18T11:16:23.684578Z","iopub.status.idle":"2023-09-18T11:16:26.807282Z","shell.execute_reply.started":"2023-09-18T11:16:23.684537Z","shell.execute_reply":"2023-09-18T11:16:26.806092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport re\n\n# Read the CSV file\ndata = pd.read_csv('/kaggle/input/content-moderation-dataset/testing.csv')\ndata['labels'] = data['class'].map({0: \"Adult content\", 1: \"Offensive Language\", 2: \"Normal Language\"})\n\n# Clean the text in the \"tweet\" column\ndata['tweet'] = data['tweet'].apply(lambda x: str(x).lower())  # Convert to lowercase\ndata['tweet'] = data['tweet'].apply(lambda x: re.sub(r'http\\S+|www\\S+|@\\S+|&\\S+|#\\S+|[^A-Za-z0-9]+', ' ', x))  # Remove URLs, mentions, hashtags, and special characters\ndata['tweet'] = data['tweet'].apply(lambda x: re.sub(r'\\s+', ' ', x).strip())  # Remove extra whitespace\n\n# Drop rows with missing values\ndata = data.dropna()\n\n# Split the data into training and testing sets\ntrain_data, test_data = train_test_split(data, test_size=0.33, random_state=42)\n\n# Collect sentences and labels from training and testing data\ntrain_sentences = train_data[\"tweet\"].values\ntrain_labels = train_data[\"labels\"].values\ntest_sentences = test_data[\"tweet\"].values\ntest_labels = test_data[\"labels\"].values\n\n# Create a tool to count words in sentences\nword_counter = CountVectorizer()\n\n# Convert sentences into word counts for training and testing\ntrain_word_counts = word_counter.fit_transform(train_sentences)\ntest_word_counts = word_counter.transform(test_sentences)\n\n# Define a function to calculate Gini impurity\ndef calculate_gini(labels):\n    unique_labels, label_counts = np.unique(labels, return_counts=True)\n    probabilities = label_counts / len(labels)\n    gini = 1 - np.sum(probabilities ** 2)\n    return gini\n\n# Define a function to find the best split\ndef find_best_split(data, labels):\n    num_samples, num_features = data.shape\n    best_gini = 1.0\n    best_split = None\n\n    for feature_idx in range(num_features):\n        feature_values = data[:, feature_idx]\n        unique_values = np.unique(feature_values)\n\n        for threshold in unique_values:\n            left_indices = np.where(feature_values <= threshold)\n            right_indices = np.where(feature_values > threshold)\n\n            left_labels = labels[left_indices]\n            right_labels = labels[right_indices]\n\n            if len(left_labels) == 0 or len(right_labels) == 0:\n                continue\n\n            gini_left = calculate_gini(left_labels)\n            gini_right = calculate_gini(right_labels)\n            weighted_gini = (len(left_labels) / num_samples) * gini_left + (len(right_labels) / num_samples) * gini_right\n\n            if weighted_gini < best_gini:\n                best_gini = weighted_gini\n                best_split = (feature_idx, threshold)\n\n    return best_split, best_gini\n\n# Define a class for a Decision Tree node\nclass DecisionTreeNode:\n    def __init__(self, depth=0, max_depth=None):\n        self.depth = depth\n        self.max_depth = max_depth\n        self.feature_idx = None\n        self.threshold = None\n        self.left = None\n        self.right = None\n        self.value = None\n\n    def fit(self, data, labels):\n        num_samples, num_features = data.shape\n        unique_labels, counts = np.unique(labels, return_counts=True)\n        self.value = unique_labels[np.argmax(counts)]\n\n        if self.depth == self.max_depth or len(unique_labels) == 1:\n            return\n\n        best_split, best_gini = find_best_split(data, labels)\n        if best_split is None or best_gini == 0:\n            return\n\n        feature_idx, threshold = best_split\n        self.feature_idx = feature_idx\n        self.threshold = threshold\n\n        left_indices = np.where(data[:, feature_idx] <= threshold)\n        right_indices = np.where(data[:, feature_idx] > threshold)\n\n        self.left = DecisionTreeNode(depth=self.depth + 1, max_depth=self.max_depth)\n        self.left.fit(data[left_indices], labels[left_indices])\n\n        self.right = DecisionTreeNode(depth=self.depth + 1, max_depth=self.max_depth)\n        self.right.fit(data[right_indices], labels[right_indices])\n\n    def predict(self, data):\n        if self.feature_idx is None:\n            return self.value\n\n        if data[self.feature_idx] <= self.threshold:\n            return self.left.predict(data)\n        else:\n            return self.right.predict(data)\n\n# Convert labels to numerical values\nlabel_mapping = {\"Adult content\": 0, \"Offensive Language\": 1, \"Normal Language\": 2}\ntrain_labels_numeric = np.array([label_mapping[label] for label in train_labels])\ntest_labels_numeric = np.array([label_mapping[label] for label in test_labels])\n\n# Build the Decision Tree\nmax_depth = 5  # You can adjust the maximum depth as needed\ndecision_tree = DecisionTreeNode(max_depth=max_depth)\ndecision_tree.fit(train_word_counts.toarray(), train_labels_numeric)\n\n# Make predictions on the training data\ntrain_predictions = np.array([decision_tree.predict(sample) for sample in train_word_counts.toarray()])\n\n# Make predictions on the testing data\ntest_predictions = np.array([decision_tree.predict(sample) for sample in test_word_counts.toarray()])\n\n\n# Convert numeric labels back to original labels\ntrain_labels_original = np.array([list(label_mapping.keys())[list(label_mapping.values()).index(label)] for label in train_labels_numeric])\ntest_labels_original = np.array([list(label_mapping.keys())[list(label_mapping.values()).index(label)] for label in test_labels_numeric])\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_labels_to_use = train_labels_numeric\ntest_labels_to_use = test_labels_numeric\n","metadata":{"execution":{"iopub.status.busy":"2023-09-18T07:32:18.077503Z","iopub.execute_input":"2023-09-18T07:32:18.078434Z","iopub.status.idle":"2023-09-18T07:32:18.083526Z","shell.execute_reply.started":"2023-09-18T07:32:18.078391Z","shell.execute_reply":"2023-09-18T07:32:18.082147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate and display training accuracy, precision, recall, and F1-score\ntrain_accuracy = accuracy_score(train_labels_to_use, train_predictions)\ntrain_precision = precision_score(train_labels_to_use, train_predictions, average='weighted')\ntrain_recall = recall_score(train_labels_to_use, train_predictions, average='weighted')\ntrain_f1 = f1_score(train_labels_to_use, train_predictions, average='weighted')\n\nprint(\"Training Metrics:\")\nprint(f\"Accuracy: {train_accuracy:.4f}\")\nprint(f\"Precision: {train_precision:.4f}\")\nprint(f\"Recall: {train_recall:.4f}\")\nprint(f\"F1 Score: {train_f1:.4f}\")\n\n# Calculate and display testing accuracy, precision, recall, and F1-score\ntest_accuracy = accuracy_score(test_labels_to_use, test_predictions)\ntest_precision = precision_score(test_labels_to_use, test_predictions, average='weighted')\ntest_recall = recall_score(test_labels_to_use, test_predictions, average='weighted')\ntest_f1 = f1_score(test_labels_to_use, test_predictions, average='weighted')\n\nprint(\"\\nTesting Metrics:\")\nprint(f\"Accuracy: {test_accuracy:.4f}\")\nprint(f\"Precision: {test_precision:.4f}\")\nprint(f\"Recall: {test_recall:.4f}\")\nprint(f\"F1 Score: {test_f1:.4f}\")\n","metadata":{"execution":{"iopub.status.busy":"2023-09-18T07:32:22.435867Z","iopub.execute_input":"2023-09-18T07:32:22.436281Z","iopub.status.idle":"2023-09-18T07:32:22.473837Z","shell.execute_reply.started":"2023-09-18T07:32:22.436245Z","shell.execute_reply":"2023-09-18T07:32:22.472710Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test=\"broke bitch cant tell me nothing\"\ndf=cv.transform([test]).toarray()\nres=clf.predict(df)\nprint(res)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-18T11:49:58.032857Z","iopub.execute_input":"2023-09-18T11:49:58.033682Z","iopub.status.idle":"2023-09-18T11:49:58.046863Z","shell.execute_reply.started":"2023-09-18T11:49:58.033637Z","shell.execute_reply":"2023-09-18T11:49:58.045830Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Naive Bayes","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport re\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.naive_bayes import MultinomialNB\n\n# Read the CSV file\ndata = pd.read_csv('/kaggle/input/content-moderation-dataset/testing.csv')\ndata['labels'] = data['class'].map({0: \"Adult content\", 1: \"Offensive Language\", 2: \"Normal Language\"})\n\n# Clean the text in the \"tweet\" column\ndata['tweet'] = data['tweet'].apply(lambda x: str(x).lower())  # Convert to lowercase\ndata['tweet'] = data['tweet'].apply(lambda x: re.sub(r'http\\S+|www\\S+|@\\S+|&\\S+|#\\S+|[^A-Za-z0-9]+', ' ', x))  # Remove URLs, mentions, hashtags, and special characters\ndata['tweet'] = data['tweet'].apply(lambda x: re.sub(r'\\s+', ' ', x).strip())  # Remove extra whitespace\n\n# Drop rows with missing values\ndata = data.dropna()\n\n# Split the data into training and testing sets\ntrain_data, test_data = train_test_split(data, test_size=0.33, random_state=42)\n\n# Collect sentences and labels from training and testing data\ntrain_sentences = train_data[\"tweet\"].values\ntrain_labels = train_data[\"labels\"].values\ntest_sentences = test_data[\"tweet\"].values\ntest_labels = test_data[\"labels\"].values\n\n# Create a tool to count words in sentences\nword_counter = CountVectorizer()\n\n# Convert sentences into word counts for training and testing\ntrain_word_counts = word_counter.fit_transform(train_sentences)\ntest_word_counts = word_counter.transform(test_sentences)\n\n# Build and train the Naive Bayes classifier\nnaive_bayes_classifier = MultinomialNB()\nnaive_bayes_classifier.fit(train_word_counts, train_labels)\n\n# Make predictions on the training data\ntrain_predictions = naive_bayes_classifier.predict(train_word_counts)\n\n# Make predictions on the testing data\ntest_predictions = naive_bayes_classifier.predict(test_word_counts)\n\n# Calculate and display training accuracy, precision, recall, and F1-score\ntrain_accuracy = accuracy_score(train_labels, train_predictions)\ntrain_precision = precision_score(train_labels, train_predictions, average='weighted')\ntrain_recall = recall_score(train_labels, train_predictions, average='weighted')\ntrain_f1 = f1_score(train_labels, train_predictions, average='weighted')\n\nprint(\"Training Metrics:\")\nprint(f\"Accuracy: {train_accuracy:.4f}\")\nprint(f\"Precision: {train_precision:.4f}\")\nprint(f\"Recall: {train_recall:.4f}\")\nprint(f\"F1 Score: {train_f1:.4f}\")\n\n# Calculate and display testing accuracy, precision, recall, and F1-score\ntest_accuracy = accuracy_score(test_labels, test_predictions)\ntest_precision = precision_score(test_labels, test_predictions, average='weighted')\ntest_recall = recall_score(test_labels, test_predictions, average='weighted')\ntest_f1 = f1_score(test_labels, test_predictions, average='weighted')\n\nprint(\"\\nTesting Metrics:\")\nprint(f\"Accuracy: {test_accuracy:.4f}\")\nprint(f\"Precision: {test_precision:.4f}\")\nprint(f\"Recall: {test_recall:.4f}\")\nprint(f\"F1 Score: {test_f1:.4f}\")\n","metadata":{"execution":{"iopub.status.busy":"2023-09-18T06:57:50.586784Z","iopub.execute_input":"2023-09-18T06:57:50.587211Z","iopub.status.idle":"2023-09-18T06:57:52.308486Z","shell.execute_reply.started":"2023-09-18T06:57:50.587159Z","shell.execute_reply":"2023-09-18T06:57:52.306569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test=\"shutup ayesha\"\ndf=cv.transform([test]).toarray()\nres=clf.predict(df)\nprint(res)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-18T11:16:42.472354Z","iopub.execute_input":"2023-09-18T11:16:42.473358Z","iopub.status.idle":"2023-09-18T11:16:42.481808Z","shell.execute_reply.started":"2023-09-18T11:16:42.473300Z","shell.execute_reply":"2023-09-18T11:16:42.480354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## EXTRA CODE","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport re\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\n# Read the CSV file\ndata = pd.read_csv('/kaggle/input/content-moderation-dataset/testing.csv')\ndata['labels'] = data['class'].map({0: \"Adult content\", 1: \"Offensive Language\", 2: \"Normal Language\"})\n\n# Clean the text in the \"tweet\" column\ndata['tweet'] = data['tweet'].apply(lambda x: str(x).lower())  # Convert to lowercase\ndata['tweet'] = data['tweet'].apply(lambda x: re.sub(r'http\\S+|www\\S+|@\\S+|&\\S+|#\\S+|[^A-Za-z0-9]+', ' ', x))  # Remove URLs, mentions, hashtags, and special characters\ndata['tweet'] = data['tweet'].apply(lambda x: re.sub(r'\\s+', ' ', x).strip())  # Remove extra whitespace\n\n# Drop rows with missing values\ndata = data.dropna()\nx = data['tweet']\ny = data['labels']\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=42)\n\n# Create a tool to count words in sentences\nword_counter = CountVectorizer()\n\n# Convert sentences into word counts for training and testing\ntrain_word_counts = word_counter.fit_transform(X_train)\ntest_word_counts = word_counter.transform(X_test)\n\n# Identify unique label categories and count the number of categories\nlabel_categories = np.unique(y_train)\nnum_categories = len(label_categories)\nnum_sentences, num_words = train_word_counts.shape\n\n# Calculate the chance of each label appearing (Prior probabilities)\nlabel_chances = {}\nfor label in label_categories:\n    label_chances[label] = np.sum(y_train == label) / len(y_train)\n\n# Calculate the chance of each word appearing given a specific label (Conditional probabilities)\nword_given_label_chances = {}\nfor label in label_categories:\n    label_word_counts = train_word_counts[y_train == label]\n    total_words_in_label = np.sum(label_word_counts)\n    word_probabilities = {}\n    \n    for word, idx in word_counter.vocabulary_.items():\n        word_occurrences = np.sum(label_word_counts[:, idx])\n        # Apply a smoothing technique to avoid division by zero\n        word_probabilities[word] = (word_occurrences + 1) / (total_words_in_label + num_words)\n    \n    word_given_label_chances[label] = word_probabilities\n\n# Make predictions on the training data\ntrain_predictions = []\nfor sentence in X_train:\n    label_probabilities = {label: np.log(label_chances[label]) for label in label_categories}\n    words = sentence.split()\n    \n    for label in label_categories:\n        for word in words:\n            if word in word_counter.vocabulary_ and word in word_given_label_chances[label]:\n                label_probabilities[label] += np.log(word_given_label_chances[label][word])\n    \n    predicted_label = max(label_probabilities, key=label_probabilities.get)\n    train_predictions.append(predicted_label)\n\n# Evaluate the model on the training data\ntrain_accuracy = accuracy_score(y_train, train_predictions)\ntrain_precision = precision_score(y_train, train_predictions, average='weighted')\ntrain_recall = recall_score(y_train, train_predictions, average='weighted')\ntrain_f1_score = f1_score(y_train, train_predictions, average='weighted')\n\n# Print training performance metrics\nprint(\"Training Accuracy:\", train_accuracy)\nprint(\"Training Precision:\", train_precision)\nprint(\"Training Recall:\", train_recall)\nprint(\"Training F1-Score:\", train_f1_score)\n\n# Make predictions on the testing data\ntest_predictions = []\nfor sentence in X_test:\n    label_probabilities = {label: np.log(label_chances[label]) for label in label_categories}\n    words = sentence.split()\n    \n    for label in label_categories:\n        for word in words:\n            if word in word_counter.vocabulary_ and word in word_given_label_chances[label]:\n                label_probabilities[label] += np.log(word_given_label_chances[label][word])\n    \n    predicted_label = max(label_probabilities, key=label_probabilities.get)\n    test_predictions.append(predicted_label)\n\n# Evaluate the model on the testing data\ntest_accuracy = accuracy_score(y_test, test_predictions)\ntest_precision = precision_score(y_test, test_predictions, average='weighted')\ntest_recall = recall_score(y_test, test_predictions, average='weighted')\ntest_f1_score = f1_score(y_test, test_predictions, average='weighted')\n\n# Print testing performance metrics\nprint(\"Testing Accuracy:\", test_accuracy)\nprint(\"Testing Precision:\", test_precision)\nprint(\"Testing Recall:\", test_recall)\nprint(\"Testing F1-Score:\", test_f1_score)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-09-18T07:40:20.247933Z","iopub.execute_input":"2023-09-18T07:40:20.248372Z","iopub.status.idle":"2023-09-18T07:41:06.325825Z","shell.execute_reply.started":"2023-09-18T07:40:20.248335Z","shell.execute_reply":"2023-09-18T07:41:06.324644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test=\"shutup ayesha\"\ndf=cv.transform([test]).toarray()\nres=clf.predict(df)\nprint(res)","metadata":{"execution":{"iopub.status.busy":"2023-09-18T07:41:08.007066Z","iopub.execute_input":"2023-09-18T07:41:08.007772Z","iopub.status.idle":"2023-09-18T07:41:08.015552Z","shell.execute_reply.started":"2023-09-18T07:41:08.007733Z","shell.execute_reply":"2023-09-18T07:41:08.014370Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to preprocess a single input text\ndef preprocess_single_text(input_text):\n    # Preprocess the input text\n    input_text = str(input_text).lower()  # Convert to lowercase\n    input_text = re.sub(r'http\\S+|www\\S+|@\\S+|&\\S+|#\\S+|[^A-Za-z0-9]+', ' ', input_text)  # Remove URLs, mentions, hashtags, and special characters\n    input_text = re.sub(r'\\s+', ' ', input_text).strip()  # Remove extra whitespace\n    return input_text\n\n# Function to predict the label for a single input text\ndef predict_single_text(input_text, label_categories, label_chances, word_given_label_chances, vectorizer):\n    # Preprocess the input text\n    input_text = preprocess_single_text(input_text)\n    \n    # Tokenize the input text\n    words = input_text.split()\n    \n    # Initialize label probabilities\n    label_probabilities = {label: np.log(label_chances[label]) for label in label_categories}\n    \n    for label in label_categories:\n        for word in words:\n            if word in vectorizer.vocabulary_ and word in word_given_label_chances[label]:\n                log_prob = np.log(word_given_label_chances[label][word])\n                label_probabilities[label] += log_prob\n    \n    # Predict the label with the highest probability\n    predicted_label = max(label_probabilities, key=label_probabilities.get)\n    return predicted_label\n\n# Example usage:\ninput_text = \"wth is that playing missy? ........ i mean seriously? RT @mr_republicann: This movie gone be trash http://t.co/8BIppVUvzr\"\npredicted_label = predict_single_text(input_text, label_categories, label_chances, word_given_label_chances, word_counter)\nprint(\"Predicted Label:\", predicted_label)\n","metadata":{"execution":{"iopub.status.busy":"2023-08-20T06:32:35.109707Z","iopub.execute_input":"2023-08-20T06:32:35.110856Z","iopub.status.idle":"2023-08-20T06:32:35.124771Z","shell.execute_reply.started":"2023-08-20T06:32:35.110804Z","shell.execute_reply":"2023-08-20T06:32:35.123585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###### ","metadata":{}}]}